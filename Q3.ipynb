{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3.1 \n",
    "\n",
    "#### Packages and functions\n",
    "convert function to convert the string to vector of word counts.\n",
    "\n",
    "cross_validation function to split train_data and train_labels into k parts.\n",
    "\n",
    "cv_modeling function perform multi-perceptron classifier on dataset with k_folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "import random\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re, string\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from statistics import mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert function to convert the string to vector of word counts\n",
    "def convert(string):\n",
    "    l = []\n",
    "    s = string.split(' ')\n",
    "    for word in lexicon:\n",
    "        c=0\n",
    "        for i in range(len(s)):\n",
    "            if s[i]==word:\n",
    "                c+=1\n",
    "        l.append(c)\n",
    "    return(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross_validation function to split train_data and train_labels into k parts\n",
    "#output format:tuple (data_list, label_list)\n",
    "#format of data and label list: array[fold_1_train, fold_2_train,...], array[fold_1_label, fold_2_label,... )]\n",
    "#format of fold: array [review1, review2,...], array [label1, label2,...]\n",
    "def cross_validation(k, x, y):\n",
    "    sample_size = round(len(y)/k)+1\n",
    "    index_list = range(len(y))\n",
    "    d_list = []\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    for i in range(k-1):\n",
    "        d_list.append(random.sample(index_list, sample_size))\n",
    "        data_list.append(x[d_list[i]])\n",
    "        label_list.append(y[d_list[i]])\n",
    "        index_list = [w for w in index_list if not w in d_list[i]]\n",
    "    d_list.append(index_list)\n",
    "    data_list.append(x[d_list[i+1]])\n",
    "    label_list.append(y[d_list[i+1]])\n",
    "    return((np.array(data_list),np.array(label_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv_modeling function perform multi-perceptron classifier on dataset with k_folds\n",
    "#return the average accuracy of k interation\n",
    "def cv_modeling(x,y,d,hi_1_nodes, activation_1):\n",
    "    acc = []\n",
    "    k = len(y)\n",
    "    for i in range(k):\n",
    "        train_idx = [w for w in range(k) if w!=i]\n",
    "        train = np.concatenate(x[train_idx])\n",
    "        labels = np.concatenate(y[train_idx])\n",
    "        inputs = tf.keras.Input(shape=(d,))\n",
    "        hi_1 = tf.keras.layers.Dense(hi_1_nodes, activation=activation_1)(inputs)\n",
    "        hi_2 = tf.keras.layers.Dense(10, activation=tf.nn.relu)(hi_1)\n",
    "        outputs = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)(hi_2)\n",
    "        model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "        history_dict = model.fit(train,\n",
    "                    labels,\n",
    "                    validation_data=(x[i],y[i]),\n",
    "                    verbose=1).history\n",
    "        acc.append(history_dict['val_acc'])\n",
    "    m = sum(sum(acc,[]))/len(acc)\n",
    "    return(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing\n",
    "Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list all the input files\n",
    "read_files_pos = glob.glob(\"train\\\\pos\\\\*.txt\")\n",
    "read_files_neg = glob.glob(\"train\\\\neg\\\\*.txt\")\n",
    "read_files = read_files_pos+read_files_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input files into train and remove punctuation and multiple spaces\n",
    "re_p = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "train = []\n",
    "train_labels = []\n",
    "for file in read_files:\n",
    "    train.append(re.sub(r' {2,}',' ', re.sub(r'[\\s]',' ',re_p.sub(' ', open(file,'r').read()))))\n",
    "    train_labels.append(1 if file in read_files_pos else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list including all unigrams\n",
    "unigram = []\n",
    "for i in range(len(train)):\n",
    "    all_words = word_tokenize(train[i].lower())\n",
    "    unigram += list(all_words)\n",
    "#select the top 10000 frequent words and remove stop words from them\n",
    "#9855 unigrams remained\n",
    "lexicon_all= [word for word, word_count in Counter(unigram).most_common(10000)]\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lexicon = [w for w in lexicon_all if not w in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the train files of string to train_data vector of word counts \n",
    "#This step takes around 5 minutes\n",
    "train_data = list(map(convert, train))\n",
    "#transform the train data (x) and train labels(y) to array.\n",
    "train_data = np.array(train_data)\n",
    "train_labels = np.array(train_labels,dtype='int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### modeling part\n",
    "model result saved in dictionary grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train_data and train_labels into k parts\n",
    "data = cross_validation(10, train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_nodes = [20, 50, 100, 300]\n",
    "list_act = {'tanh':tf.nn.tanh, 'sigmoid':tf.nn.sigmoid, 'relu':tf.nn.relu}\n",
    "grid = []\n",
    "key = []\n",
    "for i in range(len(list_nodes)):\n",
    "    for j in list_act:\n",
    "        grid.append((list_nodes[i],list_act[j]))\n",
    "        key.append((list_nodes[i],j))\n",
    "grid_search = dict.fromkeys(set(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\81468\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "WARNING:tensorflow:From C:\\Users\\81468\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "1019/1019 [==============================] - 1s 680us/sample - loss: 0.5911 - acc: 0.6791 - val_loss: 0.5046 - val_acc: 0.7719\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 0s 454us/sample - loss: 0.5995 - acc: 0.6752 - val_loss: 0.5045 - val_acc: 0.7719\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 0s 462us/sample - loss: 0.5919 - acc: 0.6968 - val_loss: 0.4852 - val_acc: 0.8070\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 511us/sample - loss: 0.6461 - acc: 0.6222 - val_loss: 0.5242 - val_acc: 0.7544\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 609us/sample - loss: 0.6054 - acc: 0.6585 - val_loss: 0.4768 - val_acc: 0.8070\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 603us/sample - loss: 0.6252 - acc: 0.6614 - val_loss: 0.5495 - val_acc: 0.7368\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 674us/sample - loss: 0.5828 - acc: 0.6781 - val_loss: 0.4802 - val_acc: 0.7719\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 595us/sample - loss: 0.6527 - acc: 0.5456 - val_loss: 0.5850 - val_acc: 0.6491\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 563us/sample - loss: 0.6033 - acc: 0.6742 - val_loss: 0.4418 - val_acc: 0.8509\n",
      "Train on 1026 samples, validate on 107 samples\n",
      "1026/1026 [==============================] - 1s 629us/sample - loss: 0.5921 - acc: 0.7057 - val_loss: 0.4852 - val_acc: 0.7477\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 713us/sample - loss: 0.6326 - acc: 0.6418 - val_loss: 0.5911 - val_acc: 0.7368\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 681us/sample - loss: 0.6591 - acc: 0.5996 - val_loss: 0.6536 - val_acc: 0.6316\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 856us/sample - loss: 0.6445 - acc: 0.6811 - val_loss: 0.6096 - val_acc: 0.7018\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 633us/sample - loss: 0.6516 - acc: 0.6614 - val_loss: 0.5827 - val_acc: 0.7895\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 782us/sample - loss: 0.6337 - acc: 0.6595 - val_loss: 0.5802 - val_acc: 0.8333\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 815us/sample - loss: 0.6533 - acc: 0.6398 - val_loss: 0.5822 - val_acc: 0.8509\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 873us/sample - loss: 0.6497 - acc: 0.6595 - val_loss: 0.5851 - val_acc: 0.7456\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 706us/sample - loss: 0.6816 - acc: 0.5751 - val_loss: 0.6181 - val_acc: 0.6667\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 760us/sample - loss: 0.7083 - acc: 0.5299 - val_loss: 0.6629 - val_acc: 0.6754\n",
      "Train on 1026 samples, validate on 107 samples\n",
      "1026/1026 [==============================] - 1s 776us/sample - loss: 0.6463 - acc: 0.6520 - val_loss: 0.5984 - val_acc: 0.7850\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 767us/sample - loss: 0.6311 - acc: 0.6379 - val_loss: 0.5773 - val_acc: 0.6842\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 773us/sample - loss: 0.6296 - acc: 0.6703 - val_loss: 0.5447 - val_acc: 0.7368\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 933us/sample - loss: 0.6382 - acc: 0.6418 - val_loss: 0.5360 - val_acc: 0.7632\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 866us/sample - loss: 0.6209 - acc: 0.6349 - val_loss: 0.5063 - val_acc: 0.7807\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 817us/sample - loss: 0.6426 - acc: 0.6163 - val_loss: 0.5372 - val_acc: 0.7368\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 965us/sample - loss: 0.6342 - acc: 0.6349 - val_loss: 0.5142 - val_acc: 0.7895\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 929us/sample - loss: 0.6147 - acc: 0.6693 - val_loss: 0.4747 - val_acc: 0.7807\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 891us/sample - loss: 0.6620 - acc: 0.6026 - val_loss: 0.5877 - val_acc: 0.7281\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 1ms/sample - loss: 0.6446 - acc: 0.6340 - val_loss: 0.5345 - val_acc: 0.7368\n",
      "Train on 1026 samples, validate on 107 samples\n",
      "1026/1026 [==============================] - 1s 1ms/sample - loss: 0.6197 - acc: 0.6472 - val_loss: 0.5102 - val_acc: 0.7757\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 1ms/sample - loss: 0.5518 - acc: 0.7193 - val_loss: 0.4554 - val_acc: 0.7807\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 1ms/sample - loss: 0.5462 - acc: 0.7105 - val_loss: 0.4577 - val_acc: 0.7807\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 1ms/sample - loss: 0.5618 - acc: 0.7085 - val_loss: 0.4742 - val_acc: 0.7456\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 1ms/sample - loss: 0.5871 - acc: 0.6771 - val_loss: 0.3830 - val_acc: 0.8509\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 1ms/sample - loss: 0.5435 - acc: 0.6977 - val_loss: 0.3766 - val_acc: 0.8421\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 1ms/sample - loss: 0.5690 - acc: 0.6928 - val_loss: 0.4683 - val_acc: 0.7982\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 1ms/sample - loss: 0.6151 - acc: 0.6624 - val_loss: 0.5149 - val_acc: 0.7895\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 1ms/sample - loss: 0.5654 - acc: 0.6977 - val_loss: 0.4411 - val_acc: 0.8246\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 1ms/sample - loss: 0.6001 - acc: 0.6820 - val_loss: 0.3851 - val_acc: 0.8246\n",
      "Train on 1026 samples, validate on 107 samples\n",
      "1026/1026 [==============================] - 2s 2ms/sample - loss: 0.5456 - acc: 0.7173 - val_loss: 0.4881 - val_acc: 0.7757\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.6108 - acc: 0.7026 - val_loss: 0.5953 - val_acc: 0.6842\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 1ms/sample - loss: 0.6277 - acc: 0.6811 - val_loss: 0.5596 - val_acc: 0.7544\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 1ms/sample - loss: 0.6317 - acc: 0.6624 - val_loss: 0.5646 - val_acc: 0.7719\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 1ms/sample - loss: 0.6117 - acc: 0.6889 - val_loss: 0.5183 - val_acc: 0.8158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 1ms/sample - loss: 0.6373 - acc: 0.6398 - val_loss: 0.5656 - val_acc: 0.8333\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 1ms/sample - loss: 0.6219 - acc: 0.7105 - val_loss: 0.5220 - val_acc: 0.8421\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 1ms/sample - loss: 0.6559 - acc: 0.6349 - val_loss: 0.6168 - val_acc: 0.7456\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 1ms/sample - loss: 0.6031 - acc: 0.6860 - val_loss: 0.5013 - val_acc: 0.8421\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.6114 - acc: 0.6879 - val_loss: 0.4799 - val_acc: 0.8421\n",
      "Train on 1026 samples, validate on 107 samples\n",
      "1026/1026 [==============================] - 2s 2ms/sample - loss: 0.6197 - acc: 0.6540 - val_loss: 0.5154 - val_acc: 0.8037\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.5995 - acc: 0.6644 - val_loss: 0.5054 - val_acc: 0.7632\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.5605 - acc: 0.7125 - val_loss: 0.4283 - val_acc: 0.7982\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.6241 - acc: 0.6634 - val_loss: 0.5394 - val_acc: 0.7544\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 1ms/sample - loss: 0.6336 - acc: 0.6546 - val_loss: 0.4818 - val_acc: 0.7807\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.5951 - acc: 0.6762 - val_loss: 0.4177 - val_acc: 0.7982\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.6163 - acc: 0.6693 - val_loss: 0.4665 - val_acc: 0.8421\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.6068 - acc: 0.6408 - val_loss: 0.4906 - val_acc: 0.7456\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.5865 - acc: 0.6820 - val_loss: 0.4173 - val_acc: 0.8421\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.6035 - acc: 0.6683 - val_loss: 0.4356 - val_acc: 0.8070\n",
      "Train on 1026 samples, validate on 107 samples\n",
      "1026/1026 [==============================] - 2s 2ms/sample - loss: 0.5937 - acc: 0.6637 - val_loss: 0.4731 - val_acc: 0.7850\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.6243 - acc: 0.6634 - val_loss: 0.5327 - val_acc: 0.7368\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.5692 - acc: 0.7056 - val_loss: 0.4662 - val_acc: 0.7895\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.6174 - acc: 0.6526 - val_loss: 0.4826 - val_acc: 0.7632\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.5404 - acc: 0.7272 - val_loss: 0.3759 - val_acc: 0.7982\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.5384 - acc: 0.7144 - val_loss: 0.3799 - val_acc: 0.8421\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.5658 - acc: 0.6968 - val_loss: 0.4054 - val_acc: 0.8596\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.5587 - acc: 0.7134 - val_loss: 0.4159 - val_acc: 0.8421\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.5610 - acc: 0.7076 - val_loss: 0.4595 - val_acc: 0.7719\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.5672 - acc: 0.7125 - val_loss: 0.4212 - val_acc: 0.7807\n",
      "Train on 1026 samples, validate on 107 samples\n",
      "1026/1026 [==============================] - 2s 2ms/sample - loss: 0.5341 - acc: 0.7300 - val_loss: 0.4017 - val_acc: 0.8411\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.5961 - acc: 0.7282 - val_loss: 0.5428 - val_acc: 0.7281\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.6251 - acc: 0.6919 - val_loss: 0.5273 - val_acc: 0.7895\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 3s 3ms/sample - loss: 0.6008 - acc: 0.7105 - val_loss: 0.5480 - val_acc: 0.7193\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.6277 - acc: 0.6605 - val_loss: 0.4883 - val_acc: 0.8333\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.6379 - acc: 0.6801 - val_loss: 0.5218 - val_acc: 0.7807\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.6709 - acc: 0.5044 - val_loss: 0.5990 - val_acc: 0.8246\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.6642 - acc: 0.6241 - val_loss: 0.5653 - val_acc: 0.7895\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.6187 - acc: 0.6605 - val_loss: 0.4831 - val_acc: 0.8421\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.5866 - acc: 0.7223 - val_loss: 0.4645 - val_acc: 0.8070\n",
      "Train on 1026 samples, validate on 107 samples\n",
      "1026/1026 [==============================] - 2s 2ms/sample - loss: 0.5808 - acc: 0.7115 - val_loss: 0.4797 - val_acc: 0.8318\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.5741 - acc: 0.6869 - val_loss: 0.4649 - val_acc: 0.7895\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 3s 3ms/sample - loss: 0.5749 - acc: 0.6850 - val_loss: 0.4287 - val_acc: 0.7982\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 3s 3ms/sample - loss: 0.5871 - acc: 0.6968 - val_loss: 0.5074 - val_acc: 0.7456\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 3s 3ms/sample - loss: 0.5549 - acc: 0.7046 - val_loss: 0.3367 - val_acc: 0.8947\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 3s 3ms/sample - loss: 0.6177 - acc: 0.6497 - val_loss: 0.4536 - val_acc: 0.8333\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 3s 3ms/sample - loss: 0.5881 - acc: 0.6919 - val_loss: 0.4351 - val_acc: 0.8509\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 3s 3ms/sample - loss: 0.5899 - acc: 0.6781 - val_loss: 0.4918 - val_acc: 0.7281\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 3s 3ms/sample - loss: 0.5592 - acc: 0.7085 - val_loss: 0.3918 - val_acc: 0.8684\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 3s 3ms/sample - loss: 0.5964 - acc: 0.6722 - val_loss: 0.3916 - val_acc: 0.8333\n",
      "Train on 1026 samples, validate on 107 samples\n",
      "1026/1026 [==============================] - 3s 3ms/sample - loss: 0.5766 - acc: 0.6881 - val_loss: 0.4017 - val_acc: 0.8692\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 4s 4ms/sample - loss: 0.5460 - acc: 0.7213 - val_loss: 0.4593 - val_acc: 0.7719\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 4s 4ms/sample - loss: 0.6063 - acc: 0.7017 - val_loss: 0.4946 - val_acc: 0.8158\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 4s 4ms/sample - loss: 0.6080 - acc: 0.6801 - val_loss: 0.5501 - val_acc: 0.7807\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 4s 4ms/sample - loss: 0.5742 - acc: 0.7046 - val_loss: 0.3257 - val_acc: 0.8509\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 4s 4ms/sample - loss: 0.5471 - acc: 0.7066 - val_loss: 0.3875 - val_acc: 0.8070\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 4s 4ms/sample - loss: 0.5405 - acc: 0.7026 - val_loss: 0.4318 - val_acc: 0.8421\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 4s 4ms/sample - loss: 0.5689 - acc: 0.7115 - val_loss: 0.4159 - val_acc: 0.8684\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 4s 4ms/sample - loss: 0.5585 - acc: 0.7282 - val_loss: 0.4124 - val_acc: 0.7982\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 4s 4ms/sample - loss: 0.5665 - acc: 0.7115 - val_loss: 0.4128 - val_acc: 0.8246\n",
      "Train on 1026 samples, validate on 107 samples\n",
      "1026/1026 [==============================] - 4s 4ms/sample - loss: 0.5288 - acc: 0.7203 - val_loss: 0.4210 - val_acc: 0.7944\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 4s 4ms/sample - loss: 0.6325 - acc: 0.7085 - val_loss: 0.5810 - val_acc: 0.7193\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 4s 4ms/sample - loss: 0.5554 - acc: 0.7203 - val_loss: 0.4547 - val_acc: 0.8333\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 5s 4ms/sample - loss: 0.6098 - acc: 0.6703 - val_loss: 0.5112 - val_acc: 0.7807\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 4s 4ms/sample - loss: 0.6356 - acc: 0.6762 - val_loss: 0.5410 - val_acc: 0.8509\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 4s 4ms/sample - loss: 0.5563 - acc: 0.7105 - val_loss: 0.4495 - val_acc: 0.7982\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 4s 4ms/sample - loss: 0.5731 - acc: 0.6977 - val_loss: 0.4151 - val_acc: 0.8684\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 4s 4ms/sample - loss: 0.5775 - acc: 0.6820 - val_loss: 0.4381 - val_acc: 0.8158\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 4s 4ms/sample - loss: 0.5727 - acc: 0.7105 - val_loss: 0.4548 - val_acc: 0.8158\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 4s 4ms/sample - loss: 0.5731 - acc: 0.7026 - val_loss: 0.4305 - val_acc: 0.8158\n",
      "Train on 1026 samples, validate on 107 samples\n",
      "1026/1026 [==============================] - 5s 5ms/sample - loss: 0.5628 - acc: 0.6998 - val_loss: 0.4399 - val_acc: 0.7944\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 4s 4ms/sample - loss: 0.5559 - acc: 0.6997 - val_loss: 0.4809 - val_acc: 0.7895\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 5s 4ms/sample - loss: 0.5442 - acc: 0.7272 - val_loss: 0.4469 - val_acc: 0.7719\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 4s 4ms/sample - loss: 0.5569 - acc: 0.7076 - val_loss: 0.5167 - val_acc: 0.7368\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 5s 5ms/sample - loss: 0.6244 - acc: 0.6467 - val_loss: 0.4636 - val_acc: 0.7632\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 5s 4ms/sample - loss: 0.6222 - acc: 0.6830 - val_loss: 0.4728 - val_acc: 0.8684\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 5s 5ms/sample - loss: 0.5257 - acc: 0.7242 - val_loss: 0.4118 - val_acc: 0.8596\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 5s 5ms/sample - loss: 0.5463 - acc: 0.7085 - val_loss: 0.4033 - val_acc: 0.8421\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 5s 5ms/sample - loss: 0.5575 - acc: 0.7174 - val_loss: 0.4400 - val_acc: 0.7719\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 6s 6ms/sample - loss: 0.6200 - acc: 0.6418 - val_loss: 0.5364 - val_acc: 0.7895\n",
      "Train on 1026 samples, validate on 107 samples\n",
      "1026/1026 [==============================] - 5s 5ms/sample - loss: 0.5413 - acc: 0.6910 - val_loss: 0.4175 - val_acc: 0.8037\n"
     ]
    }
   ],
   "source": [
    "#Takes 20 mins to run\n",
    "#perform multi-perceptron classifier on processed dataset with 10 folds for each hyper parameter\n",
    "grid_search = {}\n",
    "dim = len(lexicon)\n",
    "for i in range(len(grid)):\n",
    "    grid_search[key[i][0],key[i][1]]= cv_modeling(data[0],data[1],dim,grid[i][0],grid[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(20, 'tanh'): 0.7668716132640838,\n",
       " (20, 'sigmoid'): 0.7416625678539276,\n",
       " (20, 'relu'): 0.7512543082237244,\n",
       " (50, 'tanh'): 0.8012543082237243,\n",
       " (50, 'sigmoid'): 0.793531721830368,\n",
       " (50, 'relu'): 0.7916625738143921,\n",
       " (100, 'tanh'): 0.8025332033634186,\n",
       " (100, 'sigmoid'): 0.7945810914039612,\n",
       " (100, 'relu'): 0.8211264133453369,\n",
       " (300, 'tanh'): 0.8154041647911072,\n",
       " (300, 'sigmoid'): 0.8092638075351715,\n",
       " (300, 'relu'): 0.7996720790863037}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result, the model with 100 nodes in first hidden layer with activation function relu performs best with an accuracy of 82%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1133/1133 [==============================] - 3s 2ms/sample - loss: 0.6099 - acc: 0.6717\n"
     ]
    }
   ],
   "source": [
    "#set 100 nodes in first layer, use relu as activation function\n",
    "#train model on the total train_data\n",
    "inputs = tf.keras.Input(shape=(dim,))\n",
    "hi_1 = tf.keras.layers.Dense(100, activation=tf.nn.relu)(inputs)\n",
    "hi_2 = tf.keras.layers.Dense(10, activation=tf.nn.relu)(hi_1)\n",
    "outputs = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)(hi_2)\n",
    "model_1 = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "model_1.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history_dict = model_1.fit(train_data,\n",
    "                    train_labels,\n",
    "                    verbose=1).history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is 67% on the whole training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Packages and functions\n",
    "function word2vec_embedding to generate word2vec representation of the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate word2vec representation of the reviews \n",
    "# and remove stop words from them\n",
    "def word2vec_embedding(input_s, vec_pre):\n",
    "    vec = []\n",
    "    for i in range(len(input_s)):\n",
    "        l = [w for w in input_s[i].split(' ') if w in vec_pre and not w in stop_words]\n",
    "        vec_l = []\n",
    "        for j in range(len(l)):\n",
    "            vec_l.append(vec_pre[l[j]])\n",
    "        vec_l = sum(vec_l)/len(vec_l)\n",
    "        vec.append(vec_l)\n",
    "    return vec     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data processing\n",
    "Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list all the input files\n",
    "read_files_pos = glob.glob(\"train\\\\pos\\\\*.txt\")\n",
    "read_files_neg = glob.glob(\"train\\\\neg\\\\*.txt\")\n",
    "read_files = read_files_pos+read_files_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input files into train and remove punctuation and multiple spaces\n",
    "re_p = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "train = []\n",
    "train_labels = []\n",
    "for file in read_files:\n",
    "    train.append(re.sub(r' {2,}',' ', re.sub(r'[\\s]',' ',re_p.sub(' ', open(file,'r').read()))))\n",
    "    train_labels.append(1 if file in read_files_pos else 0)\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unzip the file to word directory\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate word2vec representation of the reviews \n",
    "train_vec = np.array(word2vec_embedding(train, word2vec), dtype='float64')\n",
    "train_labels = np.array(train_labels,dtype='int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train_data and train_labels into k parts\n",
    "data_embedding = cross_validation(10, train_vec, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_nodes = [20, 50, 100, 300]\n",
    "list_act = {'tanh':tf.nn.tanh, 'sigmoid':tf.nn.sigmoid, 'relu':tf.nn.relu}\n",
    "grid = []\n",
    "key = []\n",
    "for i in range(len(list_nodes)):\n",
    "    for j in list_act:\n",
    "        grid.append((list_nodes[i],list_act[j]))\n",
    "        key.append((list_nodes[i],j))\n",
    "grid_search = dict.fromkeys(set(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\81468\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "WARNING:tensorflow:From C:\\Users\\81468\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "1019/1019 [==============================] - 0s 401us/sample - loss: 0.6928 - acc: 0.5064 - val_loss: 0.6967 - val_acc: 0.4474\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 0s 292us/sample - loss: 0.6930 - acc: 0.5113 - val_loss: 0.6957 - val_acc: 0.3772\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 0s 297us/sample - loss: 0.6907 - acc: 0.5132 - val_loss: 0.6889 - val_acc: 0.6053\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 0s 333us/sample - loss: 0.6913 - acc: 0.5378 - val_loss: 0.6854 - val_acc: 0.6579\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 0s 364us/sample - loss: 0.6930 - acc: 0.5270 - val_loss: 0.6886 - val_acc: 0.7105\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 0s 351us/sample - loss: 0.6876 - acc: 0.5594 - val_loss: 0.6829 - val_acc: 0.6491\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 0s 371us/sample - loss: 0.6930 - acc: 0.5231 - val_loss: 0.6895 - val_acc: 0.5526\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 0s 381us/sample - loss: 0.6924 - acc: 0.5250 - val_loss: 0.6889 - val_acc: 0.5263\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 0s 363us/sample - loss: 0.6901 - acc: 0.5299 - val_loss: 0.6881 - val_acc: 0.5175\n",
      "Train on 1026 samples, validate on 107 samples\n",
      "1026/1026 [==============================] - 0s 404us/sample - loss: 0.6896 - acc: 0.5244 - val_loss: 0.6817 - val_acc: 0.6916\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 0s 414us/sample - loss: 0.6950 - acc: 0.5054 - val_loss: 0.6994 - val_acc: 0.4474\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 0s 470us/sample - loss: 0.6939 - acc: 0.5015 - val_loss: 0.7009 - val_acc: 0.3860\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 0s 468us/sample - loss: 0.6948 - acc: 0.5044 - val_loss: 0.6909 - val_acc: 0.5088\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 719us/sample - loss: 0.7348 - acc: 0.4956 - val_loss: 0.6937 - val_acc: 0.5351\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 707us/sample - loss: 0.6944 - acc: 0.4975 - val_loss: 0.6912 - val_acc: 0.5614\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 545us/sample - loss: 0.6974 - acc: 0.5015 - val_loss: 0.6941 - val_acc: 0.4912\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 515us/sample - loss: 0.6932 - acc: 0.5172 - val_loss: 0.6925 - val_acc: 0.4912\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 510us/sample - loss: 0.6948 - acc: 0.4995 - val_loss: 0.6917 - val_acc: 0.5000\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 537us/sample - loss: 0.6970 - acc: 0.5074 - val_loss: 0.6996 - val_acc: 0.4298\n",
      "Train on 1026 samples, validate on 107 samples\n",
      "1026/1026 [==============================] - 1s 543us/sample - loss: 0.8099 - acc: 0.4932 - val_loss: 0.6865 - val_acc: 0.5701\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 566us/sample - loss: 0.6920 - acc: 0.5123 - val_loss: 0.6939 - val_acc: 0.4474\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 581us/sample - loss: 0.6927 - acc: 0.5044 - val_loss: 0.6928 - val_acc: 0.5088\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 584us/sample - loss: 0.6939 - acc: 0.4946 - val_loss: 0.6919 - val_acc: 0.5175\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 602us/sample - loss: 0.6910 - acc: 0.5309 - val_loss: 0.6851 - val_acc: 0.6140\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 618us/sample - loss: 0.6924 - acc: 0.5172 - val_loss: 0.6907 - val_acc: 0.5439\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 654us/sample - loss: 0.6931 - acc: 0.5015 - val_loss: 0.6921 - val_acc: 0.4912\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 681us/sample - loss: 0.6922 - acc: 0.5339 - val_loss: 0.6891 - val_acc: 0.6316\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 677us/sample - loss: 0.6920 - acc: 0.5044 - val_loss: 0.6885 - val_acc: 0.5965\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 684us/sample - loss: 0.6919 - acc: 0.5319 - val_loss: 0.6916 - val_acc: 0.4912\n",
      "Train on 1026 samples, validate on 107 samples\n",
      "1026/1026 [==============================] - 1s 715us/sample - loss: 0.6912 - acc: 0.5439 - val_loss: 0.6989 - val_acc: 0.4299\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 952us/sample - loss: 0.6913 - acc: 0.5378 - val_loss: 0.6838 - val_acc: 0.5789\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 806us/sample - loss: 0.6897 - acc: 0.5447 - val_loss: 0.6958 - val_acc: 0.4123\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 860us/sample - loss: 0.6934 - acc: 0.4926 - val_loss: 0.6961 - val_acc: 0.5088\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 845us/sample - loss: 0.6910 - acc: 0.5044 - val_loss: 0.6928 - val_acc: 0.4649\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 847us/sample - loss: 0.6892 - acc: 0.5584 - val_loss: 0.6778 - val_acc: 0.5526\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 835us/sample - loss: 0.6913 - acc: 0.5319 - val_loss: 0.6859 - val_acc: 0.5351\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 844us/sample - loss: 0.6885 - acc: 0.5594 - val_loss: 0.6842 - val_acc: 0.6842\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 869us/sample - loss: 0.6880 - acc: 0.5545 - val_loss: 0.6807 - val_acc: 0.5965\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 862us/sample - loss: 0.6915 - acc: 0.5319 - val_loss: 0.6858 - val_acc: 0.5614\n",
      "Train on 1026 samples, validate on 107 samples\n",
      "1026/1026 [==============================] - 1s 894us/sample - loss: 0.6868 - acc: 0.5419 - val_loss: 0.6712 - val_acc: 0.6916\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 934us/sample - loss: 0.6963 - acc: 0.5162 - val_loss: 0.6982 - val_acc: 0.4474\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 931us/sample - loss: 0.6943 - acc: 0.5044 - val_loss: 0.7153 - val_acc: 0.3860\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 967us/sample - loss: 0.6989 - acc: 0.5093 - val_loss: 0.6940 - val_acc: 0.4912\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 997us/sample - loss: 0.7263 - acc: 0.4818 - val_loss: 0.6900 - val_acc: 0.5351\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 1ms/sample - loss: 0.6941 - acc: 0.4966 - val_loss: 0.6914 - val_acc: 0.5088\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 1ms/sample - loss: 0.6950 - acc: 0.4995 - val_loss: 0.6914 - val_acc: 0.5088\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 1ms/sample - loss: 0.6948 - acc: 0.4956 - val_loss: 0.6931 - val_acc: 0.4825\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 1ms/sample - loss: 0.7151 - acc: 0.5044 - val_loss: 0.6923 - val_acc: 0.5088\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 1ms/sample - loss: 0.7004 - acc: 0.5103 - val_loss: 0.6973 - val_acc: 0.4298\n",
      "Train on 1026 samples, validate on 107 samples\n",
      "1026/1026 [==============================] - 1s 1ms/sample - loss: 0.7009 - acc: 0.5000 - val_loss: 0.6985 - val_acc: 0.4299\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 1ms/sample - loss: 0.6946 - acc: 0.5025 - val_loss: 0.6945 - val_acc: 0.4474\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 1ms/sample - loss: 0.6910 - acc: 0.5417 - val_loss: 0.7044 - val_acc: 0.3860\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.6906 - acc: 0.5221 - val_loss: 0.6854 - val_acc: 0.5789\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 1ms/sample - loss: 0.6922 - acc: 0.5289 - val_loss: 0.6860 - val_acc: 0.7018\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.6913 - acc: 0.5348 - val_loss: 0.6894 - val_acc: 0.6930\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 1ms/sample - loss: 0.6894 - acc: 0.5397 - val_loss: 0.6876 - val_acc: 0.5088\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 1ms/sample - loss: 0.6897 - acc: 0.5427 - val_loss: 0.6872 - val_acc: 0.5263\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 1ms/sample - loss: 0.6912 - acc: 0.5339 - val_loss: 0.6879 - val_acc: 0.5702\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 1ms/sample - loss: 0.6932 - acc: 0.5201 - val_loss: 0.6890 - val_acc: 0.6754\n",
      "Train on 1026 samples, validate on 107 samples\n",
      "1026/1026 [==============================] - 1s 1ms/sample - loss: 0.6893 - acc: 0.5224 - val_loss: 0.6854 - val_acc: 0.6542\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 1ms/sample - loss: 0.6866 - acc: 0.5643 - val_loss: 0.6801 - val_acc: 0.6667\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.6918 - acc: 0.5221 - val_loss: 0.6831 - val_acc: 0.6404\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 1s 1ms/sample - loss: 0.6917 - acc: 0.5319 - val_loss: 0.6826 - val_acc: 0.5175\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.6893 - acc: 0.5240 - val_loss: 0.6774 - val_acc: 0.6140\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 1ms/sample - loss: 0.6899 - acc: 0.5289 - val_loss: 0.6821 - val_acc: 0.5965\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.6887 - acc: 0.5515 - val_loss: 0.6775 - val_acc: 0.6140\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.6926 - acc: 0.5368 - val_loss: 0.6868 - val_acc: 0.5702\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.6958 - acc: 0.4838 - val_loss: 0.6931 - val_acc: 0.5088\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.6897 - acc: 0.5211 - val_loss: 0.6795 - val_acc: 0.6228\n",
      "Train on 1026 samples, validate on 107 samples\n",
      "1026/1026 [==============================] - 2s 2ms/sample - loss: 0.6916 - acc: 0.5361 - val_loss: 0.7000 - val_acc: 0.4299\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.6963 - acc: 0.5172 - val_loss: 0.7024 - val_acc: 0.4474\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.6943 - acc: 0.5005 - val_loss: 0.7039 - val_acc: 0.3860\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.6934 - acc: 0.5211 - val_loss: 0.6996 - val_acc: 0.4912\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.6941 - acc: 0.5123 - val_loss: 0.6978 - val_acc: 0.4649\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.6953 - acc: 0.4907 - val_loss: 0.6868 - val_acc: 0.6579\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.6988 - acc: 0.4985 - val_loss: 0.6925 - val_acc: 0.5088\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.6927 - acc: 0.5319 - val_loss: 0.6890 - val_acc: 0.7018\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.7071 - acc: 0.5201 - val_loss: 0.6960 - val_acc: 0.4912\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.6928 - acc: 0.5280 - val_loss: 0.6891 - val_acc: 0.7368\n",
      "Train on 1026 samples, validate on 107 samples\n",
      "1026/1026 [==============================] - 2s 2ms/sample - loss: 0.7032 - acc: 0.4932 - val_loss: 0.6924 - val_acc: 0.4766\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.6916 - acc: 0.5299 - val_loss: 0.6874 - val_acc: 0.6491\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.6908 - acc: 0.5339 - val_loss: 0.6877 - val_acc: 0.5789\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.6890 - acc: 0.5270 - val_loss: 0.6783 - val_acc: 0.5263\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.6934 - acc: 0.4720 - val_loss: 0.6932 - val_acc: 0.4649\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.6914 - acc: 0.5329 - val_loss: 0.6851 - val_acc: 0.5877\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.6899 - acc: 0.5682 - val_loss: 0.6854 - val_acc: 0.7018\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.6915 - acc: 0.5240 - val_loss: 0.6886 - val_acc: 0.5088\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 6s 6ms/sample - loss: 0.6932 - acc: 0.4887 - val_loss: 0.6910 - val_acc: 0.4912\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.6930 - acc: 0.4946 - val_loss: 0.6873 - val_acc: 0.7281\n",
      "Train on 1026 samples, validate on 107 samples\n",
      "1026/1026 [==============================] - 3s 2ms/sample - loss: 0.6935 - acc: 0.5361 - val_loss: 0.6918 - val_acc: 0.4860\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.6940 - acc: 0.5240 - val_loss: 0.6933 - val_acc: 0.4386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.6865 - acc: 0.5800 - val_loss: 0.6949 - val_acc: 0.4649\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 2s 2ms/sample - loss: 0.6896 - acc: 0.5496 - val_loss: 0.6879 - val_acc: 0.4912\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 3s 3ms/sample - loss: 0.6844 - acc: 0.5751 - val_loss: 0.6645 - val_acc: 0.7105\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 3s 3ms/sample - loss: 0.6889 - acc: 0.5368 - val_loss: 0.6850 - val_acc: 0.4912\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 3s 3ms/sample - loss: 0.6951 - acc: 0.5289 - val_loss: 0.6833 - val_acc: 0.4825\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 3s 3ms/sample - loss: 0.6904 - acc: 0.5496 - val_loss: 0.6837 - val_acc: 0.5789\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 3s 3ms/sample - loss: 0.6804 - acc: 0.5937 - val_loss: 0.6604 - val_acc: 0.6930\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 3s 3ms/sample - loss: 0.6901 - acc: 0.5074 - val_loss: 0.6898 - val_acc: 0.4474\n",
      "Train on 1026 samples, validate on 107 samples\n",
      "1026/1026 [==============================] - 3s 3ms/sample - loss: 0.6953 - acc: 0.4805 - val_loss: 0.6934 - val_acc: 0.4299\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 3s 3ms/sample - loss: 0.7017 - acc: 0.4848 - val_loss: 0.7185 - val_acc: 0.4474\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 3s 3ms/sample - loss: 0.6976 - acc: 0.5221 - val_loss: 0.7016 - val_acc: 0.3860\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 3s 3ms/sample - loss: 0.6968 - acc: 0.5191 - val_loss: 0.6858 - val_acc: 0.7807\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 3s 3ms/sample - loss: 0.7028 - acc: 0.5182 - val_loss: 0.6911 - val_acc: 0.5351\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 3s 3ms/sample - loss: 0.6933 - acc: 0.5182 - val_loss: 0.6914 - val_acc: 0.5088\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 3s 3ms/sample - loss: 0.6983 - acc: 0.4740 - val_loss: 0.6910 - val_acc: 0.4912\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 3s 3ms/sample - loss: 0.7075 - acc: 0.4740 - val_loss: 0.7025 - val_acc: 0.4825\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 3s 3ms/sample - loss: 0.7124 - acc: 0.4877 - val_loss: 0.6858 - val_acc: 0.5526\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 3s 3ms/sample - loss: 0.6933 - acc: 0.5113 - val_loss: 0.6770 - val_acc: 0.5702\n",
      "Train on 1026 samples, validate on 107 samples\n",
      "1026/1026 [==============================] - 3s 3ms/sample - loss: 0.7038 - acc: 0.4854 - val_loss: 0.6897 - val_acc: 0.6542\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 3s 3ms/sample - loss: 0.6881 - acc: 0.5986 - val_loss: 0.6813 - val_acc: 0.6491\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 3s 3ms/sample - loss: 0.6900 - acc: 0.5339 - val_loss: 0.7092 - val_acc: 0.3860\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 3s 3ms/sample - loss: 0.6852 - acc: 0.5731 - val_loss: 0.6725 - val_acc: 0.5614\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 4s 4ms/sample - loss: 0.6893 - acc: 0.5564 - val_loss: 0.6789 - val_acc: 0.6667\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 3s 3ms/sample - loss: 0.6907 - acc: 0.5594 - val_loss: 0.6831 - val_acc: 0.6140\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 3s 3ms/sample - loss: 0.6899 - acc: 0.5368 - val_loss: 0.6848 - val_acc: 0.5000\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 3s 3ms/sample - loss: 0.6928 - acc: 0.5152 - val_loss: 0.6928 - val_acc: 0.4825\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 4s 4ms/sample - loss: 0.6924 - acc: 0.5113 - val_loss: 0.6886 - val_acc: 0.5614\n",
      "Train on 1019 samples, validate on 114 samples\n",
      "1019/1019 [==============================] - 4s 4ms/sample - loss: 0.6934 - acc: 0.5113 - val_loss: 0.6931 - val_acc: 0.4298\n",
      "Train on 1026 samples, validate on 107 samples\n",
      "1026/1026 [==============================] - 3s 3ms/sample - loss: 0.6891 - acc: 0.5789 - val_loss: 0.6817 - val_acc: 0.6542\n"
     ]
    }
   ],
   "source": [
    "#Takes 15 mins to run\n",
    "#perform multi-perceptron classifier on processed dataset with 10 folds for each hyper parameter\n",
    "grid_search = {}\n",
    "for i in range(len(grid)):\n",
    "    grid_search[key[i][0],key[i][1]]= cv_modeling(data_embedding[0],data_embedding[1],300,grid[i][0],grid[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(20, 'tanh'): 0.5735448390245438,\n",
       " (20, 'sigmoid'): 0.49209706485271454,\n",
       " (20, 'relu'): 0.5272011786699295,\n",
       " (50, 'tanh'): 0.5586325615644455,\n",
       " (50, 'sigmoid'): 0.4728152215480804,\n",
       " (50, 'relu'): 0.574192488193512,\n",
       " (100, 'tanh'): 0.5780783712863922,\n",
       " (100, 'sigmoid'): 0.5362600445747375,\n",
       " (100, 'relu'): 0.5722823560237884,\n",
       " (300, 'tanh'): 0.5228152126073837,\n",
       " (300, 'sigmoid'): 0.5408591538667679,\n",
       " (300, 'relu'): 0.5505082845687866}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result, the model with 100 nodes in first hidden layer with activation function tanh performs best with an accuracy of 58%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train model on the whole training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1133/1133 [==============================] - ETA: 0s - loss: 0.6867 - acc: 0.583 - 2s 2ms/sample - loss: 0.6865 - acc: 0.5852\n"
     ]
    }
   ],
   "source": [
    "#set 100 nodes in first layer, use tanh as activation function\n",
    "#train model on the total train_data\n",
    "inputs = tf.keras.Input(shape=(300,))\n",
    "hi_1 = tf.keras.layers.Dense(100, activation=tf.nn.tanh)(inputs)\n",
    "hi_2 = tf.keras.layers.Dense(10, activation=tf.nn.relu)(hi_1)\n",
    "outputs = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)(hi_2)\n",
    "model_2 = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "model_2.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history_dict = model_2.fit(train_vec,\n",
    "                    train_labels,\n",
    "                    verbose=1).history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is 59% on the whole training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3.4\n",
    "Add one more feature: normalized # of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generate new feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the length of each each review and normalize them to around 0\n",
    "length = []\n",
    "for i in range(len(train)):\n",
    "    length.append(len(train[i].split(' ')))\n",
    "avg_length = sum(length)/len(length)\n",
    "nor_length = []\n",
    "for i in range(len(length)):\n",
    "    nor_length.append(length[i]/avg_length-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the new feature to the word_embedding representation\n",
    "new_train_vec = []\n",
    "for i in range(len(train_vec)):\n",
    "    new_train_vec.append(np.append(train_vec[i], nor_length[i]))\n",
    "new_train_vec = np.array(new_train_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1133/1133 [==============================] - 2s 2ms/sample - loss: 0.6895 - acc: 0.5305\n"
     ]
    }
   ],
   "source": [
    "#set 100 nodes in first layer, use tanh as activation function\n",
    "#train model on the total train_data\n",
    "inputs = tf.keras.Input(shape=(301,))\n",
    "hi_1 = tf.keras.layers.Dense(50, activation=tf.nn.relu)(inputs)\n",
    "hi_2 = tf.keras.layers.Dense(10, activation=tf.nn.relu)(hi_1)\n",
    "outputs = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)(hi_2)\n",
    "model_3 = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "model_3.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history_dict = model_3.fit(new_train_vec,\n",
    "                    train_labels,\n",
    "                    verbose=1).history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is 53% on the whole training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3.5\n",
    "The best model above is the model_1 in Q3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Preparation\n",
    "rerun this part to get model_1 if restart python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list all the input files\n",
    "read_files_pos = glob.glob(\"train\\\\pos\\\\*.txt\")\n",
    "read_files_neg = glob.glob(\"train\\\\neg\\\\*.txt\")\n",
    "read_files = read_files_pos+read_files_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input files into train and remove punctuation and multiple spaces\n",
    "re_p = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "train = []\n",
    "train_labels = []\n",
    "for file in read_files:\n",
    "    train.append(re.sub(r' {2,}',' ', re.sub(r'[\\s]',' ',re_p.sub(' ', open(file,'r').read()))))\n",
    "    train_labels.append(1 if file in read_files_pos else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list including all unigrams\n",
    "unigram = []\n",
    "for i in range(len(train)):\n",
    "    all_words = word_tokenize(train[i].lower())\n",
    "    unigram += list(all_words)\n",
    "#select the top 10000 frequent words and remove stop words from them\n",
    "#9855 unigrams remained\n",
    "lexicon_all= [word for word, word_count in Counter(unigram).most_common(10000)]\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lexicon = [w for w in lexicon_all if not w in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the train files of string to train_data vector of word counts \n",
    "#This step takes around 5 minutes\n",
    "train_data = list(map(convert, train))\n",
    "#transform the train data (x) and train labels(y) to array.\n",
    "train_data = np.array(train_data)\n",
    "train_labels = np.array(train_labels,dtype='int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1133/1133 [==============================] - 1s 843us/sample - loss: 0.6145 - acc: 0.6567\n",
      "Epoch 2/3\n",
      "1133/1133 [==============================] - 1s 644us/sample - loss: 0.1753 - acc: 0.9532s - loss: 0.1935 - acc: \n",
      "Epoch 3/3\n",
      "1133/1133 [==============================] - 1s 652us/sample - loss: 0.0278 - acc: 0.9974\n"
     ]
    }
   ],
   "source": [
    "#set 100 nodes in first layer, use relu as activation function\n",
    "#train model on the total train_data use 5 epochs\n",
    "dim = len(lexicon)\n",
    "inputs = tf.keras.Input(shape=(dim,))\n",
    "hi_1 = tf.keras.layers.Dense(100, activation=tf.nn.relu)(inputs)\n",
    "hi_2 = tf.keras.layers.Dense(10, activation=tf.nn.relu)(hi_1)\n",
    "outputs = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)(hi_2)\n",
    "model_1 = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "model_1.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history_dict = model_1.fit(train_data,\n",
    "                    train_labels,\n",
    "                    epochs=3,\n",
    "                    verbose=1).history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input files into test and remove punctuation and multiple spaces\n",
    "read_test = glob.glob(\"test\\\\*.txt\")\n",
    "re_p = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "test = []\n",
    "for file in read_test:\n",
    "    test.append(re.sub(r' {2,}',' ', re.sub(r'[\\s]',' ',re_p.sub(' ', open(file,'r').read()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the test files of string to test_data vector of word counts \n",
    "#This step takes around 5 minutes\n",
    "test_data = list(map(convert, test))\n",
    "test_data = np.array(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_result  = model_1.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = []\n",
    "neg = []\n",
    "for i in range(len(read_test)):\n",
    "    if pred_result[i] >= 0.5:\n",
    "        pos.append(read_test[i].split('\\\\')[1])\n",
    "    else:\n",
    "        neg.append(read_test[i].split('\\\\')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output classification result\n",
    "with open('pos.txt', 'w') as f:\n",
    "    for i in range(len(pos)):\n",
    "        f.write('%s\\n' % pos[i])\n",
    "with open('neg.txt', 'w') as f:\n",
    "    for i in range(len(neg)):\n",
    "        f.write('%s\\n' % neg[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
